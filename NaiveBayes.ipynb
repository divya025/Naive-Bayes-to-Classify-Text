{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier for text\n",
    "Classifying emails into Spam or not Spam using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing : \n",
    "Prepare the data by opening the files and getting all the words in all the files in a single list (bag of words).\n",
    "We are creating two word lists one for ham and one for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def processFile(eachFile):\n",
    "    with open(eachFile, \"r+\") as theFile:\n",
    "        storeFileData = theFile.readlines()\n",
    "        wordTagList = []\n",
    "        for eachLine in storeFileData:\n",
    "            wordTagList += eachLine.split()\n",
    "            # or wordTagList += [eachLine.strip()]\n",
    "    return wordTagList\n",
    "\n",
    "def processAllFiles(directory):\n",
    "    finalWordTagList = []\n",
    "    for eachFile in os.listdir(directory):\n",
    "        finalWordTagList += processFile(directory+\"/\"+eachFile)\n",
    "    return finalWordTagList\n",
    "\n",
    "#List of all class words\n",
    "spam = processAllFiles('data/spam')\n",
    "ham = processAllFiles('data/ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected\n",
      "79088\n",
      "I\n",
      "82184\n"
     ]
    }
   ],
   "source": [
    "print(spam[6])\n",
    "print(len(spam))\n",
    "print(ham[6])\n",
    "print(len(ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Naive Bayes :\n",
    "Modify Naie Bayes to perform classification instead of getting probabailities. \n",
    "We assume that every email which has >50% chance of being a spam is classified as a spam or else it is a Ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictonary(dataList):\n",
    "    Dictionary = {}\n",
    "    for data in dataList:\n",
    "        if data in Dictionary:\n",
    "            Dictionary[data] +=1\n",
    "        else:\n",
    "            Dictionary[data] = 1\n",
    "    return Dictionary\n",
    "\n",
    "spam_map = createDictonary(spam)\n",
    "ham_map = createDictonary(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('raining', 1)\n",
      "10894\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(spam_map.items()[0])\n",
    "print(len(spam_map))\n",
    "\n",
    "\n",
    "spam_files_count = (len([name for name in os.listdir('data/spam')]))\n",
    "ham_files_count = (len([name for name in os.listdir('data/ham')]))\n",
    "\n",
    "spam_word_count = len(spam)\n",
    "ham_word_count = len(ham)\n",
    "\n",
    "print(spam_files_count)\n",
    "print(ham_files_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing on Navie Bayes\n",
    "Laplace Smoothing Applied on line 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(W|C) = (n count(w,c) + 1) / (count(c) + |V|)\n",
    "temp_dict = spam_map.copy()\n",
    "temp_dict.update(ham_map)\n",
    "#print(temp_dict)\n",
    "V = len(temp_dict)\n",
    "\n",
    "def calculateConditionalProbability(mail_map, word_count):\n",
    "    prob_map = {}\n",
    "    for each_word in mail_map.keys():\n",
    "        # P(each_word|spam)\n",
    "        p = float((mail_map[each_word] + 1))/float((word_count + V)) \n",
    "        prob_map[each_word] = p\n",
    "    return prob_map\n",
    "        \n",
    "ham_prob_map = calculateConditionalProbability(spam_map,spam_word_count)\n",
    "spam_prob_map = calculateConditionalProbability(ham_map,ham_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10894\n",
      "14000\n",
      "('looking', 0.00038577193954261296)\n"
     ]
    }
   ],
   "source": [
    "print(len(ham_prob_map))\n",
    "print(len(spam_prob_map))\n",
    "\n",
    "print(ham_prob_map.items()[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple\n",
      "19748\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the Test Data Files\n",
    "# List of all Lists is needed\n",
    "test = processAllFiles('data/test')\n",
    "print(test[6])\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a line']\n"
     ]
    }
   ],
   "source": [
    "strg = \"This is a line\"\n",
    "qwe = []\n",
    "qwe += [strg.strip()]\n",
    "print(qwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTestFile(eachFile):\n",
    "    with open(eachFile, \"r+\") as theFile:\n",
    "        storeFileData = theFile.readlines()\n",
    "        wordTagList = []\n",
    "        for eachLine in storeFileData:\n",
    "            wordTagList += eachLine.split()\n",
    "            # or wordTagList += [eachLine.strip()]\n",
    "    return wordTagList\n",
    "\n",
    "def processTestFiles(directory):\n",
    "    finalWordTagList = {}\n",
    "    for eachFile in os.listdir(directory):\n",
    "        #print(eachFile.split(\".\")[0])\n",
    "        finalWordTagList[eachFile.split(\".\")[0]] = processFile(directory+\"/\"+eachFile) \n",
    "    return finalWordTagList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('24', ['danger', 'in', 'Zimbabwe', 'This', 'land', 'problem', 'came', 'when', 'Zimbabwean', 'President', 'Mr', 'Robert', 'Mugabe', 'introduce', 'a', 'new', 'land', 'Act', 'Reform', 'wholly', 'affecting', 'the', 'rich', 'white', 'farmers', 'and', 'some', 'few', 'black', 'farmers', 'and', 'this', 'resulted', 'to', 'the', 'killing', 'and', 'mob', 'action', 'by', 'Zimbabwean', 'war', 'veterans', 'and', 'some', 'lunatics', 'in', 'the', 'society', 'In', 'fact', 'a', 'lot', 'of', 'people', 'were', 'killed', 'because', 'of', 'this', 'land', 'reform', 'Act', 'for', 'which', 'my', 'father', 'was', 'one', 'of', 'the', 'victims', 'It', 'is', 'against', 'this', 'background', 'that', 'I', 'and', 'my', 'family', 'fled', 'Zimbabwe', 'for', 'fear', 'of', 'our', 'lives', 'and', 'are', 'currently', 'staying', 'in', 'Holland', 'where', 'we', 'are', 'seeking', 'political', 'asylum', 'and', 'more', 'so', 'have', 'decided', 'to', 'transfer', 'my', 'fathers', 'money', 'to', 'a', 'more', 'reliable', 'foreign', 'account', 'Since', 'the', 'law', 'of', 'Holland', 'prohibits', 'a', 'refugee', 'asylum', 'seeker', 'to', 'open', 'any', 'bank', 'account', 'or', 'to', 'be', 'involve', 'in', 'any', 'financial', 'transaction', 'As', 'the', 'eldest', 'son', 'of', 'my', 'father', 'I', 'am', 'saddled', 'with', 'the', 'responsibility', 'of', 'seeking', 'a', 'genuine', 'foreign', 'partneraccount', 'where', 'this', 'money', 'could', 'be', 'transferred', 'without', 'the', 'knowledge', 'of', 'my', 'government', 'who', 'are', 'bent', 'on', 'taking', 'everything', 'we', 'have', 'acquired', 'Sir', 'Madam', 'If', 'you', 'are', 'willing', 'to', 'undertake', 'this', 'with', 'me', 'I', 'am', 'willing', 'to', 'give', 'you', '25', 'of', 'the', 'fund', 'for', 'your', 'assistance', 'and', 'I', 'have', 'also', 'mapped', 'out', '5', 'to', 'cover', 'any', 'expenses', 'incurred', 'from', 'the', 'transaction', 'Also', 'be', 'aware', 'the', 'balance', 'amount', 'will', 'be', 'set', 'aside', 'for', 'the', 'investment', 'through', 'you', 'and', 'the', 'profit', 'from', 'the', 'investment', 'will', 'be', 'share', '50', 'each', 'for', 'you', 'and', 'my', 'family', 'after', 'expenses', 'Please', 'contact', 'me', 'on', 'my', 'email', 'address', 'or', 'my', 'phone', 'number', 'below', 'Yours', 'sincerely', 'Mr', 'Clive', 'Osborne', '31626662624', 'Emailmrcliveosbornnetscapenet', 'reply', 'to', 'mrcliveosbornnetscapenet'])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "test = processTestFiles('data/test')\n",
    "print(test.items()[0])\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithimic Values to reduce noise in probabilities\n",
    "Apply log to find logpriors and loglikelihood and find the final probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors P(spam) & P(Ham)\n",
    "prob_spam = float(spam_files_count)/float(spam_files_count+ham_files_count)\n",
    "prob_ham = 1 - prob_spam # because it is binomial Bayes Classification\n",
    "\n",
    "import math\n",
    "prior_spam = math.log(prob_spam)\n",
    "prior_ham = math.log(prob_ham)\n",
    "    \n",
    "# P(class|Test1) is propotional to P(class) x P(word1|class) x P(word2|class) x ... x P(word_i|class)\n",
    "def calculateTestConditionalProb(prior_class, class_prob_map):\n",
    "    test_prob_map = {}\n",
    "    for file_no in test:\n",
    "        p_class_given_word = prior_class \n",
    "        for each_word in test[file_no]:\n",
    "            if(each_word in class_prob_map): # check if word exists in spam or ham class\n",
    "                p_class_given_word += math.log(class_prob_map[each_word]) \n",
    "        test_prob_map[file_no] = p_class_given_word\n",
    "    return test_prob_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24', '25', '26', '27', '20', '21', '22', '23', '28', '29', '4', '8', '59', '58', '55', '54', '57', '56', '51', '50', '53', '52', '88', '89', '82', '83', '80', '81', '86', '87', '84', '85', '3', '7', '100', '39', '38', '33', '32', '31', '30', '37', '36', '35', '34', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '2', '6', '99', '98', '91', '90', '93', '92', '95', '94', '97', '96', '11', '10', '13', '12', '15', '14', '17', '16', '19', '18', '48', '49', '46', '47', '44', '45', '42', '43', '40', '41', '1', '5', '9', '77', '76', '75', '74', '73', '72', '71', '70', '79', '78']\n",
      "['24', '25', '26', '27', '20', '21', '22', '23', '28', '29', '4', '8', '59', '58', '55', '54', '57', '56', '51', '50', '53', '52', '88', '89', '82', '83', '80', '81', '86', '87', '84', '85', '3', '7', '100', '39', '38', '33', '32', '31', '30', '37', '36', '35', '34', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '2', '6', '99', '98', '91', '90', '93', '92', '95', '94', '97', '96', '11', '10', '13', '12', '15', '14', '17', '16', '19', '18', '48', '49', '46', '47', '44', '45', '42', '43', '40', '41', '1', '5', '9', '77', '76', '75', '74', '73', '72', '71', '70', '79', '78']\n"
     ]
    }
   ],
   "source": [
    "# Choosing a class : Based on Ham and Spam Prob \n",
    "spam_test_prob_map = calculateTestConditionalProb(prob_spam, spam_prob_map)\n",
    "ham_test_prob_map = calculateTestConditionalProb(prob_ham, ham_prob_map)\n",
    "\n",
    "print(spam_test_prob_map.keys())\n",
    "print(ham_test_prob_map.keys())\n",
    "predictions_map ={}\n",
    "\n",
    "for file_no in spam_test_prob_map:\n",
    "    if(spam_test_prob_map[file_no] >= ham_test_prob_map[file_no]):\n",
    "        predictions_map[file_no] = \"Spam\"\n",
    "    else:\n",
    "        predictions_map[file_no] = \"Ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'24': 'Spam', '25': 'Spam', '26': 'Ham', '27': 'Ham', '20': 'Spam', '21': 'Spam', '22': 'Ham', '23': 'Spam', '28': 'Spam', '29': 'Ham', '4': 'Ham', '8': 'Ham', '59': 'Ham', '58': 'Spam', '55': 'Spam', '54': 'Ham', '57': 'Ham', '56': 'Ham', '51': 'Ham', '50': 'Spam', '53': 'Ham', '52': 'Ham', '88': 'Ham', '89': 'Ham', '82': 'Spam', '83': 'Ham', '80': 'Ham', '81': 'Spam', '86': 'Ham', '87': 'Ham', '84': 'Spam', '85': 'Spam', '3': 'Ham', '7': 'Ham', '100': 'Ham', '39': 'Ham', '38': 'Spam', '33': 'Ham', '32': 'Spam', '31': 'Spam', '30': 'Ham', '37': 'Spam', '36': 'Spam', '35': 'Spam', '34': 'Ham', '60': 'Ham', '61': 'Ham', '62': 'Ham', '63': 'Spam', '64': 'Ham', '65': 'Spam', '66': 'Spam', '67': 'Ham', '68': 'Spam', '69': 'Spam', '2': 'Ham', '6': 'Ham', '99': 'Ham', '98': 'Ham', '91': 'Ham', '90': 'Ham', '93': 'Ham', '92': 'Ham', '95': 'Ham', '94': 'Ham', '97': 'Ham', '96': 'Ham', '11': 'Ham', '10': 'Spam', '13': 'Spam', '12': 'Ham', '15': 'Spam', '14': 'Spam', '17': 'Spam', '16': 'Ham', '19': 'Spam', '18': 'Spam', '48': 'Ham', '49': 'Ham', '46': 'Spam', '47': 'Ham', '44': 'Spam', '45': 'Spam', '42': 'Spam', '43': 'Spam', '40': 'Spam', '41': 'Ham', '1': 'Spam', '5': 'Spam', '9': 'Ham', '77': 'Ham', '76': 'Spam', '75': 'Spam', '74': 'Spam', '73': 'Spam', '72': 'Spam', '71': 'Spam', '70': 'Spam', '79': 'Ham', '78': 'Ham'}\n"
     ]
    }
   ],
   "source": [
    "print(predictions_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email ID            Classifier Output   Truth               \n",
      "24                  Spam                Spam                \n",
      "25                  Spam                Spam                \n",
      "26                  Ham                 Ham                 \n",
      "27                  Ham                 Ham                 \n",
      "20                  Spam                Spam                \n",
      "21                  Spam                Spam                \n",
      "22                  Ham                 Ham                 \n",
      "23                  Spam                Spam                \n",
      "28                  Spam                Spam                \n",
      "29                  Ham                 Ham                 \n",
      "4                   Ham                 Ham                 \n",
      "8                   Ham                 Ham                 \n",
      "59                  Ham                 Ham                 \n",
      "58                  Spam                Spam                \n",
      "55                  Spam                Spam                \n",
      "54                  Ham                 Ham                 \n",
      "57                  Ham                 Ham                 \n",
      "56                  Ham                 Ham                 \n",
      "51                  Ham                 Spam                \n",
      "50                  Spam                Spam                \n",
      "53                  Ham                 Ham                 \n",
      "52                  Ham                 Ham                 \n",
      "88                  Ham                 Spam                \n",
      "89                  Ham                 Ham                 \n",
      "82                  Spam                Spam                \n",
      "83                  Ham                 Spam                \n",
      "80                  Ham                 Ham                 \n",
      "81                  Spam                Spam                \n",
      "86                  Ham                 Ham                 \n",
      "87                  Ham                 Spam                \n",
      "84                  Spam                Spam                \n",
      "85                  Spam                Spam                \n",
      "3                   Ham                 Ham                 \n",
      "7                   Ham                 Ham                 \n",
      "100                 Ham                 Ham                 \n",
      "39                  Ham                 Ham                 \n",
      "38                  Spam                Spam                \n",
      "33                  Ham                 Ham                 \n",
      "32                  Spam                Spam                \n",
      "31                  Spam                Spam                \n",
      "30                  Ham                 Ham                 \n",
      "37                  Spam                Spam                \n",
      "36                  Spam                Spam                \n",
      "35                  Spam                Spam                \n",
      "34                  Ham                 Ham                 \n",
      "60                  Ham                 Ham                 \n",
      "61                  Ham                 Ham                 \n",
      "62                  Ham                 Ham                 \n",
      "63                  Spam                Spam                \n",
      "64                  Ham                 Ham                 \n",
      "65                  Spam                Spam                \n",
      "66                  Spam                Spam                \n",
      "67                  Ham                 Ham                 \n",
      "68                  Spam                Spam                \n",
      "69                  Spam                Spam                \n",
      "2                   Ham                 Ham                 \n",
      "6                   Ham                 Ham                 \n",
      "99                  Ham                 Ham                 \n",
      "98                  Ham                 Ham                 \n",
      "91                  Ham                 Ham                 \n",
      "90                  Ham                 Ham                 \n",
      "93                  Ham                 Ham                 \n",
      "92                  Ham                 Ham                 \n",
      "95                  Ham                 Ham                 \n",
      "94                  Ham                 Ham                 \n",
      "97                  Ham                 Ham                 \n",
      "96                  Ham                 Ham                 \n",
      "11                  Ham                 Ham                 \n",
      "10                  Spam                Spam                \n",
      "13                  Spam                Spam                \n",
      "12                  Ham                 Ham                 \n",
      "15                  Spam                Spam                \n",
      "14                  Spam                Spam                \n",
      "17                  Spam                Spam                \n",
      "16                  Ham                 Ham                 \n",
      "19                  Spam                Spam                \n",
      "18                  Spam                Spam                \n",
      "48                  Ham                 Ham                 \n",
      "49                  Ham                 Ham                 \n",
      "46                  Spam                Spam                \n",
      "47                  Ham                 Ham                 \n",
      "44                  Spam                Spam                \n",
      "45                  Spam                Spam                \n",
      "42                  Spam                Spam                \n",
      "43                  Spam                Spam                \n",
      "40                  Spam                Spam                \n",
      "41                  Ham                 Ham                 \n",
      "1                   Spam                Spam                \n",
      "5                   Spam                Spam                \n",
      "9                   Ham                 Ham                 \n",
      "77                  Ham                 Ham                 \n",
      "76                  Spam                Spam                \n",
      "75                  Spam                Spam                \n",
      "74                  Spam                Spam                \n",
      "73                  Spam                Spam                \n",
      "72                  Spam                Spam                \n",
      "71                  Spam                Spam                \n",
      "70                  Spam                Spam                \n",
      "79                  Ham                 Ham                 \n",
      "78                  Ham                 Ham                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_table(data, cols, wide):\n",
    "    '''Prints formatted data on columns of given width.'''\n",
    "    n, r = divmod(len(data), cols)\n",
    "    pat = '{{:{}}}'.format(wide)\n",
    "    line = '\\n'.join(pat * cols for _ in range(n))\n",
    "    last_line = pat * r\n",
    "    print(line.format(*data))\n",
    "    print(last_line.format(*data[n*cols:]))\n",
    "\n",
    "\n",
    "truth_lables = {}\n",
    "with open(\"truth_lables.txt\", \"r+\") as theFile:\n",
    "    lines = theFile.readlines()\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        truth_lables[words[0]] = words[1]\n",
    "    #truth_lables.append(theFile.readlines())\n",
    "\n",
    "data = ['Email ID','Classifier Output','Truth']\n",
    "import pandas as pd\n",
    "i=1\n",
    "for file_no in predictions_map:\n",
    "    data +=[file_no]\n",
    "    data +=[predictions_map[file_no]]\n",
    "    data +=[truth_lables[file_no]]\n",
    "    \n",
    "print_table(data, 3, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Thus in binary classification, the count of true negatives is C_{0,0}, false negatives is C_{1,0}, true positives is C_{1,1} and false positives is C_{0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50,  0],\n",
       "       [ 4, 46]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for file_no in predictions_map:\n",
    "    y_pred.append(predictions_map[file_no])\n",
    "    y_true.append(truth_lables[file_no])\n",
    "    \n",
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9600\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "accuracy = sum(1 for file_no in predictions_map if predictions_map[file_no] == truth_lables[file_no]) / float(len(predictions_map))\n",
    "print(\"{0:.4f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
